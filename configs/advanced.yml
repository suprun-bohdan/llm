# Model configuration
tokenizer:
  vocab_size: 4096
  min_freq: 2
  special_tokens: ["<pad>", "<unk>", "<bos>", "<eos>"]
  max_seq_len: 256

student:
  d_model: 512
  n_heads: 8
  n_layers: 6
  max_seq_len: 256
  dropout: 0.1
  use_lora: true
  lora_rank: 8
  distill_alpha: 0.5
  temperature: 2.0
  ffn_rank: 2048
  gradient_checkpointing: true
  mixed_precision: true

# Hypernetwork configuration
use_hypernet: true
hypernet:
  hidden_sizes: [256, 512, 1024]
  activation: "gelu"

# Training configuration
batch_size: 32
gradient_accumulation_steps: 4
epochs: 10
logging_steps: 100

optimizer:
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000

# Pruning configuration
pruning:
  method: "magnitude"
  prune_rate: 0.8
  schedule: "gradual"
  start_epoch: 5
  end_epoch: 8

# Quantization configuration
quantization:
  method: "static"
  bits: 4
  symmetric: true
  per_channel: true
  start_epoch: 8

# RAG configuration
rag:
  enabled: true
  dim: 512
  nlist: 100
  m: 8
  nbits: 8

# Dataset paths
train_dataset: "data/my_dataset_clean.jsonl"
eval_dataset: "data/my_dataset_clean.jsonl"

# Output configuration
output_dir: "outputs/advanced"
checkpoint_dir: "checkpoints"
log_dir: "logs" 