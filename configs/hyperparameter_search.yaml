
model:
  d_model: 512
  n_heads: 8
  n_layers: 6
  d_ff: 2048
  dropout: 0.1
  
  use_reversible: true
  use_weight_sharing: true
  use_low_rank: true
  rank_factor: 0.25
  
  use_quantization: true
  bits: 8
  symmetric: true
  
  use_pruning: true
  pruning_threshold: 0.01
  pruning_frequency: 1000
  
  use_memory_bank: true
  memory_size: 10000
  memory_dim: 512

tokenizer:
  vocab_size: 32000
  min_freq: 2

data:
  max_length: 512
  val_size: 0.1

training:
  batch_size: 32
  num_workers: 4
  gradient_clip: 1.0

hyperparameter_search:
  metric: "val_loss"
  direction: "minimize"
  
  study_name: "transformer_search"
  
  param_space:
    d_model:
      type: "int"
      min: 256
      max: 1024
      step: 128
    
    n_heads:
      type: "int"
      min: 4
      max: 16
      step: 4
    
    n_layers:
      type: "int"
      min: 4
      max: 12
      step: 2
    
    d_ff:
      type: "int"
      min: 1024
      max: 4096
      step: 512
    
    dropout:
      type: "float"
      min: 0.1
      max: 0.5
    
    learning_rate:
      type: "float"
      min: 1e-5
      max: 1e-3
      log: true
    
    weight_decay:
      type: "float"
      min: 1e-5
      max: 1e-3
      log: true
    
    bits:
      type: "categorical"
      values: [4, 8, 16]
    
    pruning_threshold:
      type: "float"
      min: 0.001
      max: 0.1
      log: true
    
    memory_size:
      type: "int"
      min: 1000
      max: 100000
      step: 1000
    
    optimizer:
      type: "categorical"
      values: ["adam", "adamw", "sgd"]
    
    batch_size:
      type: "categorical"
      values: [16, 32, 64, 128]
  
  param_grid:
    d_model: [256, 512, 1024]
    n_heads: [4, 8, 16]
    n_layers: [4, 6, 8]
    d_ff: [1024, 2048, 4096]
    
    dropout: [0.1, 0.2, 0.3]
    
    learning_rate: [1e-5, 1e-4, 1e-3]
    weight_decay: [1e-5, 1e-4, 1e-3]
    
    bits: [4, 8, 16]
    
    pruning_threshold: [0.001, 0.01, 0.1]
    
    memory_size: [1000, 10000, 100000]
    
    optimizer: ["adam", "adamw", "sgd"]
    
    batch_size: [16, 32, 64, 128] 